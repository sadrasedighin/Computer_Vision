{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXnLVlZ6KLlO"
   },
   "source": [
    "<hr>\n",
    "<h1>\n",
    "Assignment 1\n",
    "</h1>\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "This notebook is an assignment focused on building a convolutional neural network (CNN) from scratch using NumPy. The main goal is to gain a deep understanding of the fundamental building blocks of a CNN. The notebook walks through the implementation of various layer types, including `affine (fully-connected)`, `convolutional`, `max-pooling`, and `ReLU` activation layers. For each layer, both the forward and backward passes are implemented and tested against numerical gradients to ensure correctness.\n",
    "\n",
    "Finally, these layers are assembled into a simple CNN model, which is then trained on the `MNIST` dataset to demonstrate its functionality in an image classification task. The key takeaway is to learn the inner workings of a CNN, rather than just using a high-level deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Dw8ffdlwsF3-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LHM7pMMycZW"
   },
   "source": [
    "<br><br>\n",
    "<hr>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-iEYqdZs5TM"
   },
   "source": [
    "## Affine (fully-connected) layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Efh9JsxrtBNc"
   },
   "source": [
    "### Affine forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(n*m) (c*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CXxisfyZs7l0"
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You   #\n",
    "    # will need to reshape the input into rows.                               #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    n = x.shape[0]; \n",
    "    X = x.reshape(n,-1); \n",
    "    out = X@w + b\n",
    "\n",
    "\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return out, (x, w, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqGy6mKstaKc"
   },
   "source": [
    "You can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXvlz9AUtW3r",
    "outputId": "68c6c2b4-5cad-4e15-d0de-3aa2a284a5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.769849468192957e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30NSPuRwtDuN"
   },
   "source": [
    "### Affine backward\n",
    "\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0Ik4zu_BtFeE"
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "\n",
    "    x,w,b = cache\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine backward pass.                               #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    db = dout.sum(axis=0); \n",
    "    dx = (dout @ w.T).reshape(x.shape); \n",
    "    x = x.reshape(x.shape[0],-1)\n",
    "    dw = x.T@dout;\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return dx,dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  5.399100368651805e-11\n",
      "dw error:  9.904211865398145e-11\n",
      "db error:  2.4122867568119087e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTtasVcxyW1e"
   },
   "source": [
    "<br><br>\n",
    "<hr>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1xRnZDYttX7"
   },
   "source": [
    "## Convolution layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plQoU2nltw6z"
   },
   "source": [
    "The core of a convolutional network is the convolution operation. Implement the forward and backward pass for the convolution layer.\n",
    "\n",
    "You don't have to worry too much about efficiency at this point; just write the code in whatever way you find most clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb1zsHD-yKHG"
   },
   "source": [
    "### Convolution forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SlEXDrBtxaw_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a convolutional layer.\n",
    "\n",
    "    The input consists of N data points, each with C channels, height H and\n",
    "    width W. We convolve each input with F different filters, where each filter\n",
    "    spans all C channels and has height HH and width HH.\n",
    "\n",
    "    Input:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - w: Filter weights of shape (F, C, HH, WW)\n",
    "    - b: Biases, of shape (F,)\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    - cache: (x, w, b, conv_param)\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional forward pass.                         #\n",
    "    # Hint: you can use the function np.pad for padding.                      #\n",
    "    ###########################################################################\n",
    "    N, C, H, W = x.shape\n",
    "    F, _, HH, WW = w.shape\n",
    "    stride = conv_param['stride']\n",
    "    pad = conv_param['pad']\n",
    "    # Assuming that all filters have the same size and also that all images have the same size\n",
    "    H_out = 1 + (H + 2 * pad - HH) / stride\n",
    "    W_out = 1 + (W + 2 * pad - WW) / stride\n",
    "    assert(H_out == int(H_out) and W_out == int(W_out))\n",
    "    H_out, W_out = int(H_out), int(W_out)\n",
    "\n",
    "    x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n",
    "    out = np.zeros((N, F, H_out, W_out))\n",
    "\n",
    "    # For a concrete image i, filter f, and output pixel (h_o, w_o), the value of this output pixel can be calculated\n",
    "    # as a dot product of a certain number of image samples from the image i and the weights of the filter f (assuming\n",
    "    # that we straighten them out into 1 dimension using ravel()). Note that as we change (h_o, w_o) but are still in the\n",
    "    # same image i and filter f, the weights remain constant but the image samples change and they are given by a sliding\n",
    "    # window looking into the image i.\n",
    "\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    for f in range(F): \n",
    "        for n in range(N): \n",
    "            for i in range(H_out): \n",
    "                for j in range(W_out): \n",
    "                    h_start = i*stride; \n",
    "                    h_end = h_start + HH; \n",
    "                    w_start = j*stride; w_end = w_start + WW; \n",
    "                    out[n][f][i][j] = np.sum(x_pad[n,:,h_start:h_end,w_start:w_end]*w[f])+b[f]\n",
    "\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    cache = (x_pad, w, b, conv_param)\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfPTFT6AxbGt"
   },
   "source": [
    "You can test your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_naive\n",
      "difference:  2.2121476417505994e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4) # N,C,H,W\n",
    "w_shape = (3, 3, 4, 4) # F, C, HH, WW\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]])\n",
    "\n",
    "# Compare your output to ours; difference should be around 2e-8\n",
    "print('Testing conv_forward_naive')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YXu_wisySAf"
   },
   "source": [
    "### Convolution backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CWmjxslayTfF"
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives. (N, F, H_out, W_out)\n",
    "    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x (N, C, H, W)\n",
    "    - dw: Gradient with respect to w (F, C, HH, WW)\n",
    "    - db: Gradient with respect to b (F,)\n",
    "    \"\"\"\n",
    "    x_pad, w, b, conv_param = cache\n",
    "    stride = conv_param['stride']\n",
    "    pad = conv_param['pad']\n",
    "    N = x_pad.shape[0]\n",
    "    F, C, HH, WW = w.shape\n",
    "    _, _, H_out, W_out = dout.shape\n",
    "    # We keep the padding on dx so that we can keep the same indexing scheme for x_pad as for dx\n",
    "    dx, dw, db = np.zeros_like(x_pad), np.zeros_like(w), np.zeros_like(b)\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional backward pass.                        #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # db\n",
    "    # We sum over all channels, width and height but not over the axis 1 which indexes the kernels\n",
    "    for i in range(F): db[i] = np.sum(dout[:,i,:,:]); \n",
    "    # dw\n",
    "    # The partial derivative of loss w.r.t. a weight of a filter f is calculated as a sum over all the input images i\n",
    "    # of all the pixels that were multiplied by this weight and thus contributed to the value\n",
    "    # of an output pixel at (i, f, h_o, w_o). This is  multiplied by dl/d(out[i, f, h_o, w_o]) i.e.,\n",
    "    # the partial derivative of the loss w.r.t a single output pixel.\n",
    "    #\n",
    "    H_out,W_out = dout.shape[2],dout.shape[3];\n",
    "    for f in range(F): \n",
    "        for n in range(N): \n",
    "            for i in range(H_out): \n",
    "                for j in range(W_out): \n",
    "                    h_start = i*stride; \n",
    "                    h_end = h_start + HH; \n",
    "                    w_start = j*stride; w_end = w_start + WW; \n",
    "                    grad_out = dout[n][f][i][j];\n",
    "                    dw[f] += x_pad[n,:,h_start:h_end,w_start:w_end]*grad_out;\n",
    "                    dx[n,:,h_start:h_end,w_start:w_end]+=w[f]*grad_out;\n",
    "    if (pad>0): dx = dx[:,:,pad:-pad,pad:-pad];\n",
    "    # dx\n",
    "    # The derivative of the loss w.r.t. an original pixel in an image i is the sum over all output pixels\n",
    "    # in the output image i, over all filters f and of all the weights\n",
    "    # in these filters that contributed to the value of the output pixel at (i, f, h_o, w_o). This is multiplied by\n",
    "    # dl/d(out[i, f, h_o, w_o]).\n",
    "\n",
    "\n",
    "    # Remove padding from dx\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****#\n",
    "    return dx, dw, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKMtv9isyVUG"
   },
   "source": [
    "You can test your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward_naive function\n",
      "db error:  3.3726153958780465e-11\n",
      "dw error:  2.2471264748452487e-10\n",
      "dx error:  1.159803161159293e-08\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# Your errors should be around 1e-8'\n",
    "print('Testing conv_backward_naive function')\n",
    "print('db error: ', rel_error(db, db_num))\n",
    "print('dw error: ', rel_error(dw, dw_num))\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAiNscXwyTx-",
    "outputId": "c0974ac3-afb5-4557-b032-adc8d2a1fa49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward_naive function\n",
      "db error:  2.1494967362289156e-11\n",
      "dw error:  5.185597891706744e-10\n",
      "dx error:  2.9516763408862005e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# Your errors should be around 1e-8'\n",
    "print('Testing conv_backward_naive function')\n",
    "print('db error: ', rel_error(db, db_num))\n",
    "print('dw error: ', rel_error(dw, dw_num))\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajgWmmGdyV2e"
   },
   "source": [
    "<br><br>\n",
    "<hr>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCQ7N6gS0EsG"
   },
   "source": [
    "## Max pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19JV5Tsg0HZn"
   },
   "source": [
    "Implement the forward and backward pass for the max-pooling operation in the function. Again, don't worry too much about computational efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4MswB9b0O5-"
   },
   "source": [
    "### Max pooling forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CLzacyFN0kWW"
   },
   "outputs": [],
   "source": [
    "\n",
    "def max_pool_forward_naive(x, pool_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a max pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C, H, W)\n",
    "    - pool_param: dictionary with the following keys:\n",
    "      - 'pool_height': The height of each pooling region\n",
    "      - 'pool_width': The width of each pooling region\n",
    "      - 'stride': The distance between adjacent pooling regions\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data\n",
    "    - cache: (x, pool_param)\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the max pooling forward pass                            #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    stride = pool_param['stride'];\n",
    "    h =pool_param['pool_height'];\n",
    "    w =pool_param['pool_width']; \n",
    "    N,C,W,H = x.shape; \n",
    "    W_out = (W-w+2)//stride;\n",
    "    H_out = (H-h+2)//stride; \n",
    "    out = np.zeros((N,C,W_out,H_out));\n",
    "    for n in range(N): \n",
    "        for c in range(C): \n",
    "            for i in range(H_out): \n",
    "                for j in range(W_out): \n",
    "                    h_start = i*stride; h_end = h_start+h; \n",
    "                    w_start = j*stride; w_end = w_start+w;\n",
    "                    out[n][c][i][j] = np.max(x[n,c,h_start:h_end,w_start:w_end]);\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    cache = (x, pool_param)\n",
    "    return out, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vN6Go3cG0Msd"
   },
   "source": [
    "Check your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_forward_naive function:\n",
      "difference:  4.1666665157267834e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# Compare your output with ours. Difference should be around 1e-8.\n",
    "print('Testing max_pool_forward_naive function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MO53oGH0MSl",
    "outputId": "f379412b-3cdb-44a0-c0c1-3abf032c4f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_forward_naive function:\n",
      "difference:  4.1666665157267834e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# Compare your output with ours. Difference should be around 1e-8.\n",
    "print('Testing max_pool_forward_naive function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmCBers00Tq6"
   },
   "source": [
    "### Max pooling backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_fDfVFQd0Tq7"
   },
   "outputs": [],
   "source": [
    "\n",
    "def max_pool_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a max pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives (N, C, H_out, W_out)\n",
    "    - cache: A tuple of (x, pool_param) as in the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x (N, C, H, W)\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the max pooling backward pass                           #\n",
    "    ###########################################################################\n",
    "    x, pool_param = cache\n",
    "    N, C, H_out, W_out = np.shape(dout)\n",
    "    pool_height = pool_param['pool_height']\n",
    "    pool_width = pool_param['pool_width']\n",
    "    stride = pool_param['stride']\n",
    "    H_out,W_out = dout.shape[2],dout.shape[3];\n",
    "    dx = np.zeros_like(x) # (N, C, H, W)\n",
    "\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    for n in range(N): \n",
    "        for c in range(C): \n",
    "            for i in range(H_out): \n",
    "                for j in range(W_out):\n",
    "                    h_start = i*stride; h_end = h_start+pool_height; \n",
    "                    w_start = j*stride; w_end = w_start+pool_width;\n",
    "                    mx = np.argmax(x[n,c,h_start:h_end,w_start:w_end]);\n",
    "                    ii,jj = divmod(mx,pool_height);\n",
    "                    dx[n][c][h_start+ii][w_start+jj]+=dout[n][c][i][j];\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_CGKDkR0Tq7"
   },
   "source": [
    "Check your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vkXUmc4x0Tq7",
    "outputId": "5cafef12-402b-4167-8453-d97f4e231778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_backward_naive function:\n",
      "dx error:  3.27562514223145e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(3, 2, 8, 8) # (N, C, H, W)\n",
    "dout = np.random.randn(3, 2, 4, 4) # (N, C, H_out, W_out)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
    "\n",
    "out, cache = max_pool_forward_naive(x, pool_param)\n",
    "dx = max_pool_backward_naive(dout, cache)\n",
    "\n",
    "# Your error should be around 1e-12\n",
    "print('Testing max_pool_backward_naive function:')\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM-VCbDh4cp-"
   },
   "source": [
    "<br><br>\n",
    "<hr>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ATa-T_sK-uH"
   },
   "source": [
    "## ReLU activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBO9A9bTLNUY"
   },
   "source": [
    "### ReLU forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "A8P4naV8G0aE"
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                  #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    cache = x.copy(); \n",
    "    mask = x>=0; \n",
    "    out = np.zeros_like(x); \n",
    "    out[mask] = x[mask]; \n",
    "    out[~mask] = 0;\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhmxMIyIMDir"
   },
   "source": [
    "Check your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61RiWYc35Ndd",
    "outputId": "76cbfa09-2aa6-42cb-e125-8f68ae3b1f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.999999798022158e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc0U3W5dLQiJ"
   },
   "source": [
    "### ReLU backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "_hj5SxR6G3Hs"
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                 #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    x = cache; \n",
    "    dx =(x>=0).astype(float); \n",
    "    #(n*m)*(n*m)\n",
    "    dx = dx*dout;\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OY7uyBBxMEP6"
   },
   "source": [
    "Check your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.2756349136310288e-12\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBeUG_ePHDju"
   },
   "source": [
    "<br><br>\n",
    "<hr>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKGF_QadLanp"
   },
   "source": [
    "## Simple multi layer network for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "zxarDYEPFUG0"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        np.random.seed(231) # for reproducibility\n",
    "\n",
    "        # Hardcoded network dimensions\n",
    "        self.num_classes = 10\n",
    "        self.hidden_dim = 1024\n",
    "\n",
    "        # Layer shapes and parameters initialization\n",
    "        self.conv_param1 = {'stride': 2, 'pad': 1}\n",
    "        self.W1 = 0.01 * np.random.randn(8, 1, 4, 4) # First convolutional layer weights\n",
    "        self.b1 = np.zeros(8) # First convolutional layer biases\n",
    "\n",
    "        self.conv_param2 = {'stride': 1, 'pad': 1}\n",
    "        self.W2 = 0.01 * np.random.randn(16, 8, 3, 3) # Second convolutional layer weights\n",
    "        self.b2 = np.zeros(16) # Second convolutional layer biases\n",
    "\n",
    "        self.W3 = 0.01 * np.random.randn(16 * 7 * 7, self.hidden_dim)  # Fully-connected layer weights\n",
    "        self.b3 = np.zeros(self.hidden_dim) # Fully-connected layer biases\n",
    "\n",
    "        self.W4 = 0.01 * np.random.randn(self.hidden_dim, self.num_classes) # Output layer weights\n",
    "        self.b4 = np.zeros(self.num_classes) # Output layer biases\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass through the network\n",
    "        out1, self.cache1 = conv_forward_naive(X, self.W1, self.b1, self.conv_param1) # Conv1\n",
    "        relu_out1, self.relu_cache1 = relu_forward(out1) # ReLU\n",
    "\n",
    "        out2, self.cache2 = conv_forward_naive(relu_out1, self.W2, self.b2, self.conv_param2) # Conv2\n",
    "        relu_out2, self.relu_cache2 = relu_forward(out2) # ReLU\n",
    "\n",
    "        pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "        out3, self.cache3 = max_pool_forward_naive(relu_out2, pool_param) # Max pooling\n",
    "\n",
    "        self.cache_reshape = out3.shape\n",
    "        out3_flat = out3.reshape(out3.shape[0], -1) # Flatten for fully-connected layer\n",
    "\n",
    "        out4, self.cache4 = affine_forward(out3_flat, self.W3, self.b3) # Fully-connected layer\n",
    "\n",
    "        relu_out4, self.relu_cache4 = relu_forward(out4) # ReLU\n",
    "\n",
    "        scores, self.cache5 = affine_forward(relu_out4, self.W4, self.b4) # Output layer\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def backward(self, scores, y):\n",
    "        # Backward pass to compute gradients\n",
    "        loss, grads = 0, {}\n",
    "\n",
    "        N = scores.shape[0]\n",
    "        shifted = scores - np.max(scores, axis=1, keepdims=True) # for numerical stability\n",
    "        probs = np.exp(shifted) / np.sum(np.exp(shifted), axis=1, keepdims=True) # Softmax probabilities\n",
    "\n",
    "        loss = -np.sum(np.log(probs[np.arange(N), y])) / N # Cross-entropy loss\n",
    "\n",
    "        dscores = probs.copy()\n",
    "        dscores[np.arange(N), y] -= 1\n",
    "        dscores /= N # Gradient of the loss with respect to scores\n",
    "\n",
    "        dx5, dW4, db4 = affine_backward(dscores, self.cache5) # Backprop through output layer\n",
    "        grads['W4'], grads['b4'] = dW4, db4\n",
    "\n",
    "        drelu4 = relu_backward(dx5, self.relu_cache4) # Backprop through ReLU\n",
    "        dx4, dW3, db3 = affine_backward(drelu4, self.cache4) # Backprop through fully-connected layer\n",
    "        grads['W3'], grads['b3'] = dW3, db3\n",
    "\n",
    "        dx3 = dx4.reshape(self.cache_reshape) # Reshape back to pooled feature map shape\n",
    "        dpool = max_pool_backward_naive(dx3, self.cache3) # Backprop through max pooling\n",
    "\n",
    "        drelu2 = relu_backward(dpool, self.relu_cache2) # Backprop through ReLU\n",
    "        dx2, dW2, db2 = conv_backward_naive(drelu2, self.cache2) # Backprop through Conv2\n",
    "        grads['W2'], grads['b2'] = dW2, db2\n",
    "\n",
    "        drelu1 = relu_backward(dx2, self.relu_cache1) # Backprop through ReLU\n",
    "        dx1, dW1, db1 = conv_backward_naive(drelu1, self.cache1) # Backprop through Conv1\n",
    "        grads['W1'], grads['b1'] = dW1, db1\n",
    "        return loss, grads\n",
    "\n",
    "\n",
    "    def update(self, grad, learning_rate):\n",
    "        # Update model parameters using gradients\n",
    "        for param in ['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4']:\n",
    "            self.__dict__[param] -= learning_rate * grad[param]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vp60Wn8SR2Jg"
   },
   "source": [
    "<br><br>\n",
    "<hr>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-U5GKiEMKlS"
   },
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eiEhDA6QPsn"
   },
   "source": [
    "### Initialize network and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "iNgkKyITQPdf"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "batch_size = 20\n",
    "simple_model = SimpleCNN()\n",
    "\n",
    "max_data = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahFFhi_NQL9g"
   },
   "source": [
    "### load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "7ZHa8Y_bMNUJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9.91M/9.91M [00:33<00:00, 298kB/s]\n",
      "100%|█████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 151kB/s]\n",
      "100%|█████████████████████████████████████| 1.65M/1.65M [00:01<00:00, 999kB/s]\n",
      "100%|████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 4.49MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing steps\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                      # Converts to [0,1] range\n",
    "    transforms.Normalize((0.5,), (0.5,))        # Normalization to [-1,1]\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainset = torch.utils.data.Subset(trainset, range(max_data))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmolheujQWqX"
   },
   "source": [
    "### run training\n",
    "The goal was to train the model on an MNIST subset. If the loss decreases, your implementation works—no need for optimal performance here. This confirms basic functionality; further tuning can come later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "I56bC3VPNFPW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Loss: 11.5146\n",
      "Epoch 02, Loss: 11.4963\n",
      "Epoch 03, Loss: 11.4822\n",
      "Epoch 04, Loss: 11.4661\n",
      "Epoch 05, Loss: 11.4549\n",
      "Epoch 06, Loss: 11.4428\n",
      "Epoch 07, Loss: 11.4322\n",
      "Epoch 08, Loss: 11.4218\n",
      "Epoch 09, Loss: 11.4122\n",
      "Epoch 10, Loss: 11.4055\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, lr, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for i, (X_batch, y_batch) in enumerate(dataloader):\n",
    "            # Convert tensors to numpy arrays\n",
    "            X_batch = X_batch.numpy()\n",
    "            y_batch = y_batch.numpy()\n",
    "\n",
    "            scores = model.forward(X_batch)\n",
    "            loss, grads = model.backward(scores, y_batch)\n",
    "\n",
    "            model.update(grads, lr)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "train(simple_model, trainloader, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
