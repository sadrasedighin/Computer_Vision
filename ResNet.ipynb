{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3879dda5-0efa-4d4e-bfbb-7ceeac69d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader,random_split,Dataset\n",
    "from torchvision import transforms \n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73d17cfb-0865-4b34-84ff-405847e1aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module): \n",
    "    def __init__(self, in_channel,out_channel,down_sample=None,stride=1): \n",
    "        super().__init__(); \n",
    "        self.net = nn.Sequential(nn.Conv2d(in_channel,out_channel,kernel_size=1,padding=0),\n",
    "                                 nn.BatchNorm2d(out_channel),nn.ReLU(),\n",
    "                                 nn.Conv2d(out_channel,out_channel,kernel_size=3,padding=1,\n",
    "                                          stride=stride),\n",
    "                                 nn.BatchNorm2d(out_channel),nn.ReLU(),\n",
    "                                 nn.Conv2d(out_channel,out_channel*4,kernel_size=1,padding=0),\n",
    "                                 nn.BatchNorm2d(out_channel*4),nn.ReLU()); \n",
    "        self.down_sample = down_sample; \n",
    "        self.relu = nn.ReLU();\n",
    "    def forward(self, x): \n",
    "        identity = x; \n",
    "        x = self.net(x); \n",
    "        if (self.down_sample is not None): identity = self.down_sample(identity); \n",
    "        x = x.add(identity)\n",
    "        x = self.relu(x); \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee35627a-3afd-46a1-be64-435aa11e50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module): \n",
    "    def __init__(self,layers,in_channles,num_class): \n",
    "        super().__init__(); \n",
    "        self.conv1 = nn.Conv2d(in_channles,64,kernel_size=7,padding=3,stride=2); \n",
    "        self.bn1 = nn.BatchNorm2d(64); \n",
    "        self.relu = nn.ReLU(); \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,padding=1,stride=2); \n",
    "        self.in_channels = 64;\n",
    "        self.layer1 = self._make_layer(layers[0],64,stride = 1); \n",
    "        self.layer2 = self._make_layer(layers[1],128,stride = 2);\n",
    "        self.layer3 = self._make_layer(layers[2],256,stride = 2);\n",
    "        self.layer4 = self._make_layer(layers[3],512,stride = 2);\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1));\n",
    "        self.fc = nn.Linear(512*4,num_class); \n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x); \n",
    "        x = self.bn1(x); \n",
    "        x = self.relu(x); \n",
    "        x = self.maxpool(x); \n",
    "        x = self.layer1(x); \n",
    "        x = self.layer2(x); \n",
    "        x = self.layer3(x); \n",
    "        x = self.layer4(x); \n",
    "        x = self.avgpool(x) ;\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        x = self.fc(x); \n",
    "        return x; \n",
    "    def _make_layer(self,num_layers,out_channels,stride): \n",
    "        down_sample = None;\n",
    "        if (stride!=1 or self.in_channels!=out_channels*4): \n",
    "            down_sample = nn.Sequential(nn.Conv2d(self.in_channels,out_channels*4,kernel_size=1,\n",
    "                                                  stride=stride)); \n",
    "        layers = [] \n",
    "        layers.append(Block(self.in_channels,out_channels,stride=stride,down_sample=down_sample))\n",
    "        self.in_channels = out_channels * 4; \n",
    "        for i in range(num_layers-1): \n",
    "            layers.append(Block(self.in_channels,out_channels)); \n",
    "        return nn.Sequential(*layers); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14306832-b70a-44af-a3e0-0a6d150aa4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 170M/170M [45:51<00:00, 62.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomVerticalFlip(p=0.5), \n",
    "                                transforms.RandomAutocontrast(p=0.5), \n",
    "                                transforms.ToTensor()]); \n",
    "test_transform = transforms.ToTensor();\n",
    "train_dataset = datasets.CIFAR10(root='torchvision_datasets',train=True,download=True,transform=transform);\n",
    "test_dataset = datasets.CIFAR10(root='torchvision_datasets',train=False,download=True,transform=test_transform);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3048ecaf-57ef-49c8-8a35-7a3681260f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True); \n",
    "test_loader = DataLoader(test_dataset,batch_size=64,shuffle=False); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50737b00-145c-4ef7-891d-95d4f7c5f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(in_channles=3,num_class = 10,layers = [2,2,2,2]); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a73e68ea-a695-4345-aa7a-21687745b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lr = 5e-3,params = model.parameters()); \n",
    "loss_function = nn.CrossEntropyLoss(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8942305-ae44-4fa1-ad31-52195c2abcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2969\n",
      "22.6337\n",
      "12.0756\n",
      "6.1198\n",
      "4.4569\n",
      "3.1194\n",
      "3.3161\n",
      "3.7425\n",
      "5.0826\n",
      "6.0931\n",
      "5.9051\n",
      "4.7354\n",
      "7.174\n",
      "6.4609\n",
      "3.8481\n",
      "2.349\n",
      "2.7132\n",
      "2.6722\n",
      "2.5559\n",
      "5.1292\n",
      "3.2615\n",
      "4.9757\n",
      "2.46\n",
      "2.7303\n",
      "2.9017\n",
      "2.7393\n",
      "3.852\n",
      "3.2703\n",
      "3.5351\n",
      "2.7802\n",
      "3.4843\n",
      "3.8105\n",
      "3.5447\n",
      "3.0366\n",
      "2.6091\n",
      "2.7476\n",
      "2.7916\n",
      "3.2117\n",
      "2.4037\n",
      "2.4342\n",
      "2.9224\n",
      "2.8669\n",
      "3.0854\n",
      "2.4702\n",
      "2.5968\n",
      "3.0339\n",
      "2.3108\n",
      "2.7963\n",
      "2.6759\n",
      "2.9463\n",
      "2.4476\n",
      "3.0055\n",
      "2.3487\n",
      "2.6877\n",
      "2.7269\n",
      "2.4196\n",
      "2.4093\n",
      "2.3793\n",
      "2.4036\n",
      "2.8644\n",
      "3.5368\n",
      "2.3798\n",
      "3.4934\n",
      "2.4525\n",
      "3.3094\n",
      "2.8138\n",
      "2.6966\n",
      "3.2439\n",
      "2.5255\n",
      "3.4303\n",
      "2.8934\n",
      "3.1222\n",
      "2.5088\n",
      "2.5757\n",
      "3.0219\n",
      "2.5496\n",
      "2.4881\n",
      "2.7805\n",
      "2.2499\n",
      "3.2172\n",
      "2.3858\n",
      "2.6199\n",
      "2.3916\n",
      "2.9201\n",
      "2.8668\n",
      "3.513\n",
      "2.9319\n",
      "3.4524\n",
      "2.9456\n",
      "3.5821\n",
      "2.3858\n",
      "2.3121\n",
      "3.2952\n",
      "2.3451\n",
      "2.4732\n",
      "2.8227\n",
      "2.9837\n",
      "2.2479\n",
      "2.7014\n",
      "2.4235\n",
      "2.5619\n",
      "2.3223\n",
      "2.4832\n",
      "2.4968\n",
      "2.2098\n",
      "2.6177\n",
      "2.6166\n",
      "2.5352\n",
      "2.2422\n",
      "2.297\n",
      "2.2601\n",
      "2.4597\n",
      "2.7128\n",
      "2.4589\n",
      "2.5733\n",
      "2.2828\n",
      "2.72\n",
      "2.3249\n",
      "2.2471\n",
      "2.9196\n",
      "2.4517\n",
      "2.289\n",
      "2.4843\n",
      "3.6014\n",
      "2.5218\n",
      "3.2533\n",
      "2.545\n",
      "2.3468\n",
      "2.3639\n",
      "2.3949\n",
      "2.3002\n",
      "2.5067\n",
      "2.5707\n",
      "2.3952\n",
      "3.6532\n",
      "2.7054\n",
      "2.5519\n",
      "2.6792\n",
      "2.3939\n",
      "2.4092\n",
      "2.2969\n",
      "2.5552\n",
      "2.4147\n",
      "2.4144\n",
      "2.2978\n",
      "2.2721\n",
      "2.2057\n",
      "2.493\n",
      "2.1786\n",
      "2.3238\n",
      "2.8027\n",
      "2.6273\n",
      "2.129\n",
      "2.2397\n",
      "3.8502\n",
      "2.3821\n",
      "2.4733\n",
      "2.6602\n",
      "2.8097\n",
      "2.275\n",
      "2.3226\n",
      "2.3542\n",
      "2.2423\n",
      "2.458\n",
      "2.4755\n",
      "2.866\n",
      "2.2607\n",
      "2.7414\n",
      "2.5583\n",
      "2.3763\n",
      "2.195\n",
      "2.455\n",
      "2.3174\n",
      "2.1661\n",
      "2.2658\n",
      "2.1198\n",
      "2.3939\n",
      "2.3448\n",
      "2.1071\n",
      "2.4069\n",
      "2.1533\n",
      "2.0144\n",
      "2.1879\n",
      "2.2706\n",
      "2.1322\n",
      "2.3588\n",
      "2.0829\n",
      "2.1329\n",
      "2.2334\n",
      "2.4406\n",
      "2.1495\n",
      "2.2476\n",
      "2.3198\n",
      "2.3947\n",
      "2.4887\n",
      "2.2402\n",
      "2.5853\n",
      "2.3862\n",
      "2.1398\n",
      "2.3244\n",
      "2.2553\n",
      "2.4358\n",
      "2.2353\n",
      "2.0279\n",
      "2.2682\n",
      "2.1037\n",
      "2.5039\n",
      "2.8748\n",
      "2.5247\n",
      "2.2435\n",
      "2.2104\n",
      "2.3362\n",
      "2.1985\n",
      "2.0437\n",
      "2.5263\n",
      "2.4638\n",
      "2.4038\n",
      "2.111\n",
      "2.9648\n",
      "2.6566\n",
      "2.5392\n",
      "2.4804\n",
      "2.0789\n",
      "1.9172\n",
      "2.5052\n",
      "2.545\n",
      "2.1149\n",
      "1.9983\n",
      "2.3836\n",
      "2.1822\n",
      "2.2876\n",
      "2.0958\n",
      "2.4261\n",
      "2.3105\n",
      "2.0387\n",
      "2.1342\n",
      "2.3897\n",
      "2.289\n",
      "2.234\n",
      "2.468\n",
      "1.9993\n",
      "1.881\n",
      "1.973\n",
      "2.1347\n",
      "2.3006\n",
      "2.5728\n",
      "2.421\n",
      "2.7995\n",
      "1.9405\n",
      "2.156\n",
      "2.1882\n",
      "2.1652\n",
      "2.0584\n",
      "2.4574\n",
      "1.989\n",
      "2.3889\n",
      "2.2372\n",
      "2.0319\n",
      "2.2686\n",
      "2.0102\n",
      "2.0336\n",
      "2.4197\n",
      "2.1361\n",
      "2.0382\n",
      "2.2711\n",
      "2.2009\n",
      "2.026\n",
      "2.0363\n",
      "2.1202\n",
      "2.2762\n",
      "1.9859\n",
      "2.0236\n",
      "2.2141\n",
      "2.4534\n",
      "1.925\n",
      "2.7369\n",
      "2.3854\n",
      "2.3135\n",
      "2.1612\n",
      "2.3861\n",
      "2.2371\n",
      "2.6104\n",
      "2.0228\n",
      "2.3363\n",
      "2.0023\n",
      "1.9241\n",
      "2.0014\n",
      "1.9656\n",
      "2.1249\n",
      "2.2988\n",
      "2.1688\n",
      "2.738\n",
      "2.268\n",
      "1.9617\n",
      "2.0262\n",
      "2.7108\n",
      "2.3335\n",
      "2.0424\n",
      "1.991\n",
      "2.3393\n",
      "2.1613\n",
      "2.089\n",
      "2.1945\n",
      "2.1962\n",
      "2.404\n",
      "2.3202\n",
      "2.0515\n",
      "2.0523\n",
      "2.0925\n",
      "2.2575\n",
      "2.2878\n",
      "2.0188\n",
      "2.2418\n",
      "2.0929\n",
      "1.9744\n",
      "2.2623\n",
      "2.0795\n",
      "2.2819\n",
      "2.6174\n",
      "1.9404\n",
      "2.0225\n",
      "2.1229\n",
      "2.2616\n",
      "2.0676\n",
      "1.9117\n",
      "2.151\n",
      "2.1415\n",
      "1.9306\n",
      "2.1505\n",
      "2.1552\n",
      "2.1044\n",
      "2.0371\n",
      "2.0389\n",
      "2.1389\n",
      "2.181\n",
      "2.057\n",
      "2.0415\n",
      "1.9836\n",
      "2.1211\n",
      "2.0757\n",
      "2.0535\n",
      "2.0524\n",
      "2.1188\n",
      "2.4721\n",
      "2.3191\n",
      "1.9883\n",
      "2.3346\n",
      "2.1348\n",
      "2.1452\n",
      "2.4394\n",
      "2.8228\n",
      "2.1701\n",
      "2.0693\n",
      "2.0372\n",
      "2.2783\n",
      "2.1507\n",
      "2.1816\n",
      "2.1031\n",
      "2.2637\n",
      "2.6997\n",
      "2.1088\n",
      "1.9063\n",
      "2.3241\n",
      "2.041\n",
      "2.2148\n",
      "1.8954\n",
      "2.2082\n",
      "2.2614\n",
      "2.2156\n",
      "1.9991\n",
      "2.2197\n",
      "1.8905\n",
      "2.4761\n",
      "2.1703\n",
      "2.2234\n",
      "2.0128\n",
      "1.9044\n",
      "2.0706\n",
      "2.2296\n",
      "2.0702\n",
      "2.443\n",
      "2.2764\n",
      "2.3656\n",
      "2.0733\n",
      "2.4527\n",
      "2.1052\n",
      "2.0023\n",
      "2.2077\n",
      "2.2625\n",
      "2.0406\n",
      "2.229\n",
      "2.1497\n",
      "3.0089\n",
      "2.7152\n",
      "2.2676\n",
      "2.24\n",
      "2.1444\n",
      "2.0511\n",
      "2.3075\n",
      "2.8715\n",
      "3.332\n",
      "2.3032\n",
      "2.8602\n",
      "2.2357\n",
      "2.1268\n",
      "2.0558\n",
      "2.7355\n",
      "2.7356\n",
      "2.0093\n",
      "2.1362\n",
      "2.663\n",
      "2.178\n",
      "2.1398\n",
      "2.1738\n",
      "2.1323\n",
      "2.4092\n",
      "2.0332\n",
      "2.3334\n",
      "2.0838\n",
      "2.6883\n",
      "2.4349\n",
      "2.0666\n",
      "2.0744\n",
      "2.2847\n",
      "2.1828\n",
      "2.0995\n",
      "2.0295\n",
      "2.1688\n",
      "2.1953\n",
      "2.2993\n",
      "1.944\n",
      "2.2822\n",
      "2.2194\n",
      "2.1177\n",
      "2.1469\n",
      "2.0906\n",
      "2.2041\n",
      "1.9818\n",
      "1.9948\n",
      "2.0525\n",
      "2.4662\n",
      "2.0993\n",
      "2.187\n",
      "1.9682\n",
      "2.0111\n",
      "2.111\n",
      "2.1133\n",
      "2.0962\n",
      "2.0022\n",
      "2.0239\n",
      "2.6802\n",
      "2.0238\n",
      "2.0109\n",
      "2.2515\n",
      "1.9263\n",
      "2.0341\n",
      "2.3172\n",
      "2.1442\n",
      "2.0948\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m out \u001b[38;5;241m=\u001b[39m model(x); \n\u001b[0;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(out,y); \n\u001b[1;32m----> 7\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m; \n\u001b[0;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep(); \n\u001b[0;32m      9\u001b[0m b\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m;\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train();\n",
    "b = 0; \n",
    "for x, y in train_loader : \n",
    "    optimizer.zero_grad(); \n",
    "    out = model(x); \n",
    "    loss = loss_function(out,y); \n",
    "    loss.backward(); \n",
    "    optimizer.step(); \n",
    "    b+=1;\n",
    "    print(round(loss.item(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5574f839-83b9-44c0-98fb-f82910bd4bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32768//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6165c1-db43-4014-bf4f-3bb74698c39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7220a-d81c-44d1-88ec-f2365e290a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671708a-0532-4bfc-af89-050d0a1ebe64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276046d-5bf4-4c16-918c-5e49b7903fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
